load("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/bigTDM all 3 texts NO stopWordRemoval.RData")
triTDM<-removeSparseTerms(triTDM,0.96)
# 2. Isolate bigrams and unigrams within trigrams
#=============================================#
# Get total frequency in corpus of each trigram
library(slam)
gramCount<-as.matrix(row_sums(triTDM))
# Create dataframe frequency tables
freqTable <- data.frame(gram=dimnames(gramCount)[[1]],count=gramCount,stringsAsFactors=FALSE)
gc()
library(tm)
triTDM<-removeSparseTerms(triTDM,0.96)
# Get total frequency in corpus of each trigram
library(slam)
gramCount<-as.matrix(row_sums(triTDM))
# Create dataframe frequency tables
freqTable <- data.frame(gram=dimnames(gramCount)[[1]],count=gramCount,stringsAsFactors=FALSE)
# sort descending the frequency tables
freqTable<-freqTable[order(-freqTable$count),]
# Split corpus trigrams up to words
words <- strsplit(freqTable$gram," ")
# Set first two words as an attribute, the trigram prediction query pair of words
freqTable$triquery <- sapply(words,FUN=function(x) paste(x[1],x[2]))
# Set each word of trigram as an attribute for future use
freqTable$one <- sapply(words,FUN=function(x) paste(x[1]))
freqTable$two <- sapply(words,FUN=function(x) paste(x[2]))
freqTable$three <- sapply(words,FUN=function(x) paste(x[3]))
# 3. INPUT MUNGING
#=============================================#
## INPUT MUNGING ##
# i. Take an input:
input<-list("The guy in front of me just bought a pound of bacon, a bouquet, and a case of","You're the reason why I smile everyday. Can you follow me please? It would mean the","Hey sunshine, can you follow me and make me the","Very early observations on the Bills game: Offense still struggling but the","Go on a romantic date at the","Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my","Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some","After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little","Be grateful for the good times and keep the faith during the","If this isn't the cutest thing you've ever seen, then you must be")
# ii. Perform Transformations.
input<-makeCorpus(input)
input<-lapply(input, FUN=function(x){as.character(x[1])})
# iii. Reduce to last two words
input<-lapply(input,FUN=function(x){unlist(strsplit(x,"\\s+"))})
two<-lapply(input,FUN=length)
one<-lapply(two,FUN=function(x){x-1})
# iv. set querying bigrams and unigrams we will search for in trigrams and bigrams respectively
bigram<-list(lapply(1:length(input),FUN=function(x){paste(input[[x]][one[[x]]],input[[x]][two[[x]]])}))
bigram<-bigram[[1]]
unigram<-list(lapply(1:length(input),FUN=function(x){paste(input[[x]][two[[x]]])}))
unigram<-unigram[[1]]
# 4. FIND PREDICTION MATCHES
#=============================================#
#TRIGRAMS:
# Find trigrams where first two words match and put in matches list
trimatches<-lapply(1:length(bigram), FUN=function(x){freqTable[freqTable$triquery == bigram[[x]],]})
bimatch1 <- lapply(1:length(unigram), FUN=function(x){freqTable[freqTable$one == unigram[[x]],]})
bimatch2 <- lapply(1:length(unigram), FUN=function(x){freqTable[freqTable$two == unigram[[x]],]})
# Put these results in a frequency table and rank them as such.
matches<-lapply(1:length(input), FUN=function(x){c(trimatches[[x]]$three,bimatch1[[x]]$two,bimatch2[[x]]$one)})
matchCorpus<-lapply(1:length(matches),FUN=function(x){makeCorpus(matches[[x]])}) # (Corpus(VectorSource(matches)) caused very similar terms to be considered separately like singular and plural)
matchTDM<-lapply(1:length(matchCorpus),FUN=function(x){TermDocumentMatrix(matchCorpus[[x]])})
# Get total frequency in prediction corpus of each prediction
predCount<-lapply(1:length(matchTDM),FUN=function(x){rowSums(as.matrix(matchTDM[[x]]))})
# Create dataframe frequency table
predFreq <- lapply(1:length(predCount),FUN=function(x){data.frame(gram=names(predCount[[x]]),count=predCount[[x]],stringsAsFactors=FALSE)})
predFreq<-lapply(1:length(predFreq),FUN=function(x){predFreq[[x]][order(-predFreq[[x]]$count),]})
# If predictions are more than 10, reduce to 10
predFreq<-lapply(1:length(predFreq),FUN=function(x){if(nrow(predFreq[[x]])>10){predFreq[[x]]<-predFreq[[x]][1:10,]}})
sink('predictions.txt')
predFreq
sink()
sink('trimatches.txt')
trimatches
sink()
gc()
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/")
# FUNCTION DEFINITIONS #
library(tm)
# Make Corpus and do transformations only
makeCorpus<- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
return(corpus)
}
# TrigramTokenizer function
library(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# Make Corpus, Transform, Make Trigram TDM
makeTriTDM <- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
tdm<- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
#tdm<-removeSparseTerms(tdm,0.97)
return(tdm)}
## DATA MUNGING ##
# 1. Corpus, transformations, and TDM Creation
#=============================================#
fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=20000
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}
# Read, chunk, parse data, then make corpus, do transformations, make TDM of tri-grams:
test<-fileMunge("testData1.txt")
triTDM <- makeTriTDM(test)
inspect(triTDM)
findAssocs(triTDM,"are",0)
findAssocs(triTDM,"are",1)
findAssocs(triTDM,"Talking heads are my favorite band",0)
findAssocs(triTDM,"Talk head are",0)
findAssocs(triTDM,"talk head are",0)
fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=1
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}
# Read, chunk, parse data, then make corpus, do transformations, make TDM of tri-grams:
test<-fileMunge("testData1.txt")
triTDM <- makeTriTDM(test)
# rm(test)
# gc()
#1. Calculate probabilities of each line in my testdata1.txt
# "Talking heads are my favorite band"
findAssocs(triTDM,"Talking heads are my favorite band",0)
findAssocs(triTDM,"talk head are",0)
findAssocs(triTDM,"talk head are my",0)
corp<-makeCorpus(test)
corp[1]
corp[[1]
]
corp[[1]][1]
typeof(corp[[1]][1])
typeof(corp[[1]][1][1])
typeof(corp$content[[1]][1])
corp$content[[1]][1]
corp$content[1]
corp$content[[1]]
findAssocs(triTDM,corp[[1]],0)
as.character(corp[[1]])
findAssocs(triTDM,as.character(corp[[1]]),0)
TrigramTokenizer(corp[[1]])
train<-TrigramTokenizer(corp[[1]])
train
train[1]
findAssocs(triTDM,train,0)
findAssocs(triTDM,train[1],0)
typeof(train)
list(train)
findAssocs(triTDM,list(train),0)
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,train[x],0)}
)
trainGrams
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,x,0)})
trainGrams
triTDM
row_sums(triTDM)
library(slam)
row_sums(triTDM)
row_sums(triTDM)[1]
Matrix(test)
?Matrix
triTDM[1]
triTDM[[1]]
summary(triTDM)
triTDM
triTDM$Terms
triTDM[1]$Terms
attributes(triTDM)
names(triTDM)
names(triTDM$dimnames)
names(triTDM$dimnames$Terms)
names(triTDM$dimnames[1])
names(triTDM$dimnames[1][1])
names(triTDM$dimnames[[1]])
names(triTDM$dimnames[1])
triTDM$dimnames[1]
triTDM$dimnames[1][1]
triTDM$dimnames[[1]]
triTDM$dimnames[[1]][2]
triTDM[1,]
triTDM[2,]
?data.frame
triTDM$dimnames[[1]]
typeof(triTDM$dimnames[[1]])
freq<-data.frame(trigrams=triTDM$dimnames[[1]])
freq
freq[1]
dim(freq)
findFreqTerms(triTDM)
?findFreqTerms
inspect(triTDM)
typeof(inspect(triTDM))
inspect(triTDM)[1]
inspect(triTDM)[2]
inspect(triTDM)[[1]]
inspect(triTDM$terms)
inspect(triTDM$Terms)
inspect(triTDM)
dimnames(triTDM)
dimnames(triTDM)$Docs
tdm["are my favorite"]
tdm["are my favorit"]
triTDM["are my favorit"]
triTDM["are my favorit",]
triTDM["are my favorit",dimnames(triTDM)$Docs]
inspect(triTDM)
z<-inspect(triTDM)
typeof(z)
rowSums(z)
rowSums(z)[1]
rowSums(z)["are my favorit"]
rowSums(z)["band are playing"]
z
rowSums(inspect(triTDM))
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM))
)
freq
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM))
)
freq
rowSums(inspect(triTDM)
)
nrows(freq)
nrow(freq)
freq$MLE<-(freq$count/nrow(freq))
freq
