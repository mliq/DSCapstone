Post-Tokenization
====
# Current operation

So moving to a sentence based model, how would this work?

1. probability of every trigram.

2. Take a sentence and find probability of every trigram in it.

3. Fine, but where is the TREE? Where does it combine all these things?

idea:
use which to identify which documents each term from our query is found in, then examine all the other words in THOSE documents...
[tf-idf](http://en.wikipedia.org/wiki/Tf–idf)
Well, that is actually what findAssoc does right?


I would take each trigram in THAT sentence, find probabilities, multiply them together.

Then, given a prediction query, find the sentences that are most similar.

Then look at the sentence with highest probability.

then look at the words not used yet.

Let's try with our test data:

```r
gc()
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/")

# FUNCTION DEFINITIONS #
library(tm)
# Make Corpus and do transformations only
makeCorpus<- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
return(corpus)
}

# TrigramTokenizer function
library(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

# Make Corpus, Transform, Make Trigram TDM
makeTriTDM <- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
tdm<- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
#tdm<-removeSparseTerms(tdm,0.97)
return(tdm)}

## DATA MUNGING ##

# 1. Corpus, transformations, and TDM Creation
#=============================================#

fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=1
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks 
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}

# Read, chunk, parse data, then make corpus, do transformations, make TDM of tri-grams:
test<-fileMunge("testData1.txt")
triTDM <- makeTriTDM(test)
# rm(test)
# gc()

#
corp<-makeCorpus(test)
findAssocs(triTDM,as.character(corp[[1]]),0)


#1. Calculate probabilities of each line in my testdata1.txt
# "Talking heads are my favorite band"
findAssocs(triTDM,"Talking heads are my favorite band",0)
findAssocs(triTDM,"talk head are",0)
                #talk head are
# are my favorit              1
# head are my                 1
# my favorit band             1
findAssocs(triTDM,"talk head are my",0)

corp<-makeCorpus(test)
findAssocs(triTDM,as.character(corp[[1]]),0)
TrigramTokenizer(corp[[1]])
#[1] "talk head are"  
#[2] "head are my"    
#[3] "are my favorit" 
#[4] "my favorit band"

#make list of the trigrams in that sentence
train<-TrigramTokenizer(corp[[1]])

# Make a frequency table with <UNK> and also every trigram we have found.


# Create a list of all trigrams and their frequency count.
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM)) #stringsAsFactors=FALSE ?? 
freq$MLE<-(freq$count/nrow(freq))


#make list of associations for each trigram in that sentence
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,x,0)})

### PROBABILITIES ###

#So for each training sentence, we take each trigram it has, then calculate the count of that trigram over the total number of trigrams in our corpus.
#Access each part of TDM: triTDM$dimnames[[1]][2]

So...
findAssoc doesn't really help, it's not a frequency or probability table.

library(slam)

gramCount<-as.matrix(row_sums(triTDM))

```

## I think what's really missing though is relations
I am able to follow the model of trigram modeling but it's still just an expanded bag of words/probability model. 
I want something that says, a sentence with "offense" may also have "defense".

### Wordnet thesaurus, FreeBase, DBPedia

# So it seems like the main thing I am missing in my model is, 
*identifying sentences*
THe key seems to be grouping a sentence to find it's probability, though I'm not quite sure why?

# Question 4 is a key case where trigrams won't cut it!
Question 4
Very early observations on the Bills game: Offense still struggling but the

You need to catch Offense and see what words are common with that.

## I'm trying Model5 with .97 and no stopwords, 
however I believe I will need to redo my model big-time based on 
Stanford NLP.
And work in a smaller test set for this I think.
ERROR, too large (16.7 million still in TDM)

but HMM the one that was created before is quite good!
Now I'm not positive if that was from .97 but I think so... 
it was created at 4:47...

OK so, it must come from this run, because I canceled .985 before it completed.
Yet, jeez i dunno i guess it went through somewhat somehow...

Anyway it looks quite good:

1. beer
2. world
3. well, here it's lost and plus it didn't cut down to 10 for some reason...

so what the hell i'll try again at .95?

trying .96, which has nrows=815930

## I am a bit mystified why this piece of slow running code is needed

```r
# Put these results in a frequency table and rank them as such.
matches<-lapply(1:length(input), FUN=function(x){c(trimatches[[x]]$three,bimatch1[[x]]$two,bimatch2[[x]]$one)})
matchCorpus<-lapply(1:length(matches),FUN=function(x){makeCorpus(matches[[x]])}) # (Corpus(VectorSource(matches)) caused very similar terms to be considered separately like singular and plural)
matchTDM<-lapply(1:length(matchCorpus),FUN=function(x){TermDocumentMatrix(matchCorpus[[x]])})
```
I am finding which words in the prediction corpus are most common....
but what is the point of that...]
why not simply use the freqTable$count??

# OK I need to go through this stanford NLP more carefully

I do not understand how they make bigrams. They skip a word?
Seems like maybe bigrams use stopword elimination but trigrams and unigrams do not.

# [intro n-grams](https://class.coursera.org/nlp/lecture/14)

1. we should be able to calculate the joint probability of an entire sentence by P(A)P(B|A)P(C|A,B) etc. (chain rule)

2. Markov assumption skips over words to generate probabilities, perhaps just the first and last two words of a sentence.

# [Estimating n-gram probabilities](https://class.coursera.org/nlp/lecture/128)

1. bigram estimates:
*First, key note that they use <s> and </s> so that start and end of sentence are considered part of a bigram.*

if sentence is "A B C D E"
P(A|<s>)\*P(B|A)\*P(C|B)...P(</s|E)

so here it's all bigrams.

use log probabilities because adding them is faster than multiplying.

## [log probability](http://en.wikipedia.org/wiki/Log_probability)
Since the log of a number in [0,1] is negative, negative log probabilities are more commonly used.

x' in probability means negation of an event, so the probability it does NOT happen (1-P)
-

SRILM toolkit, google n-gram
# [Evaluation and Perplexity](https://class.coursera.org/nlp/lecture/129)

Perplexity is the probability of the test set, normalized by the number of words.

Normalization is done by putting the probability to the -1/N power, where N is the number of bigrams in a sentence (same as number of words? Not if using tokens it would be words+1.
Idea being that many words are less likely...

Perplexity is inversely related to probability.
"How many things could occur next?""
average branching factor

So of course, the tree/branching idea would be that we are reducing the perplexity as it goes along.

perplexity = weighted equivalent branching factor

so PP(x)=P(x)^(-1/N)

# [Generalization and Zeros](https://class.coursera.org/nlp/lecture/17)

Any zero probability bigram (simply not in the test corpus), will be assigned 0 probability and cause the sentence to be 0 probability, and perplexity uncalculable (divide by zero)

# [Smoothing: Add One](https://class.coursera.org/nlp/lecture/18)

We need to take away from counted bigrams if we want to add to 0 bigrams to keep ratios the same.

Simplest way is Laplace smoothing - just add 1 to everything.

MLE Maximum Likelihood Estimate - the one that maximizes the likelihood of the training set T given the model M - just the count of something over the total number. key point here is just that it is for a training corpus, may be totally off in another corpus.

MLE LM (language model) assigns highest probability to the training corpus thus has lowest perplexity. Smoothed PP will be higher.

We DO NOT USE add-1 smoothing for n-grams because there are TOO many zeroes, it totally skews our data.

# [INTERPOLATION / BACKOFF](https://class.coursera.org/nlp/lecture/19)

Backoff means - just use trigram if you have a good match, but otherwise stepdown to bigrams 
(this is the model I'm using now I believe)

Interpolation says use a mix of unigram, bigram, trigram always.
Lambda must sum to 1.
So with 3 types of grams, we have lambda=1/3 for each.

So the linear interpolation probability model is:
P(Sam|I am) = P(Sam)/3+P(Sam|am)/3+P(Sam|I am)/3

in other words, we take the probability of the trigram over all sentences, and this is lambda. THen we multiply the unigram, bigram and trigram all by that... ? *Or is it 1/3 because of 3 types of grams??*

P(Sam) = #Sam/#total WORDS (unigrams)
P(Sam|am) = #S
P(Sam|I am) = #(Sam I am)/#ALL SENTENCES.

Choose lambdas that maximize probability of held-out data (like a second test set...)

so 1. fix n-gram probabilities on training data
2. search for lambdas that give largest probability to held-out set

	log P(w1...wn)|M(lambda1...lambdak) = Sigma log PM(lamba1...lambdaK(w1...wn)

## OOV = out of vocabulary words
create unknown word token <UNK>

So we filter out low-probability words, a fixed lexicon L of size V.

then any words not in training set are <UNK>

## Pruning

only keep n-grams with count>threshold
remove singletons of higher-order n-grams
entropy-based pruning - advanced.

## Back-off

Stupid back-off - not probabilities, scores (greater than 1 is possible). Only relative frequencies multiplied by a constant rate.

So if trigram occurs, I use that, if not, I use bigram probability.
I don't discount whether the probability comes from bigrams or trigrams.

Stupid back-off good for very large n-grams. (web-scale)

Extended interpolated kneser-ney - more used, discussed later.

## Advanced Language Models
Discriminative models - choose n-gram weights to improve a task.

Parsing-based models - discussed later

Caching models - recently used words more likely 

#[Good-Turing Smoothing](https://class.coursera.org/nlp/lecture/32)
1:41

## Lambdas conditional on previous words??

# ok here is something to focus on.

Question 6 is "on my way" (probably).
I get no trigrams because, I assume, on and my get eliminated.
Perhaps stopword elimination is a problem.

[](http://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html)
# problem: i'm not really using library(slam)! would that help??
actually row_sums IS slam package.
A better question though is, could I somehow keep those rowsums in sparse matrix or other format more friendly than regular matrix?

# backup all corpus in TDM: 
C:\Users\Michael\SkyDrive\Code\GitHub\DSCapstone\Coursera-SwiftKey\final\en_US\triTDM all corpus no sparse.RData
(27 million rows!)

bigTDM all 3 texts NO stopWordRemoval.RData
30415033 rows.

*.98* down to 327837

# Question: what exactly is SwiftKey contributing to this??

# Model 6
It would be best if you understand what you WANT to do first on an intuitive level before searching for the programmatic way to do it.

One option is to identify the subject and verb of a sentence, then make a model which matches those with other words...

Damn, that crashed.
perhaps removeSparse .99 or .98 would work.
ALSO, could do removeSparse AFTER big TDM is made, because crash occured here:

```r
# Set each word of trigram as an attribute for future use
freqTable$one <- sapply(words,FUN=function(x) paste(x[1]))
```

## beyond ngrams.pdf
probabilistic context-free grammars (PCFGs)

# Model 5

# try one more time WITHOUT remove sparse!

## Model5.works.R Review
Trigrams predicts better than bigrams.

1. NA 
2. worlds
3. day,happi
4. Not working
5. Not good
6. NA
7. time
8. NA
9. not sure, is wore = worse?
10. NA

Problems in predictions.txt

1. beer has now disappeared..

Next steps:

## 1. break up the file in R instead of unix?

```r
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone")
twit<-list()
fileName="testData1.txt"
text<-read.table(fileName, stringsAsFactors=FALSE)

# wait, do i need to break it up, or could I just feed the corpus in chunks? well whats the difference really...

totalLines=dim(text)[1]
chunkSize=2
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder

i=1
line=1
while (line<totalLines){
end=line+chunkSize-1
twit[[i]]<-text[line:end,]
line=end+1
i=i+1
}
twit[[i]]<-text[line:totalLines,]

```

## 2. increase corpus size by entering the rest of shit?


# Model 4

So remember that removeSparse 0.98 got the best results so far.
(right?) Try 0.97.

*.97: 154180 TDM rows*
						 gram count
just         just    14
anyon       anyon     2
beer         beer     2
case         case     2
divorc     divorc     2
alreadi   alreadi     1
benjamin benjamin     1
best         best     1
build       build     1
classic   classic     1

OK great, that improved results further.
let's try .96?

*.96 100333 rows*
					 gram count
just       just     9
case       case     2
beer       beer     1
best       best     1
classic classic     1
concern concern     1
curious curious     1
divorc   divorc     1
got         got     1
great     great     1

-In a way this is better because case can be eliminated as being in our query...
-And, perhaps just can be eliminated...

*.98*
				 gram count
just     just    16
case     case     4
miss     miss     3
anyon   anyon     2
beer     beer     2
divorc divorc     2
ever     ever     2
ltd       ltd     2
phone   phone     2
studi   studi     2

## Next

1. Investigate weighting options?

2. How does tokenizer work? Is it getting every group?

3. I think what would really improve is to recognize the verb of sentence... Is it always the second word? (No...)

4. How can machine learning and trees be implemented?

5. https://www.google.com/search?q=beyond+ngrAms&ie=UTF-8&oe=UTF-8&hl=en&client=safari

6. https://www.google.com/search?q=word+tree+predict&ie=UTF-8&oe=UTF-8&hl=en&client=safari

****
# OK, Model 3.3 has produced the two frequency tables, now how to combine them?

First reduce it to 10 options.

```r
# If predictions are more than 10, reduce to 10
if(nrow(predFreq)>10){
predFreq<-predFreq[1:10,]
}

# Get the frequency of each predFreq gram in our 'bag of words' freqTable
uniPreds<-uniFreqTable[which(uniFreqTable$gram %in% predFreq$gram),]

```
OK this made my results WORSE though.
The deal is, I want beer to be the result.

I get this:
				 gram count
just     just    16
case     case     4
miss     miss     3
anyon   anyon     2
beer     beer     2
divorc divorc     2
ever     ever     2
ltd       ltd     2
phone   phone     2
studi   studi     2

now just could maybe be eliminated...

> uniPreds
				 gram  count
just     just 149907
miss     miss  29680
ever     ever  24989
anyon   anyon  14857
phone   phone  12481
beer     beer   6930
case     case   4397
studi   studi   4040
divorc divorc    399

## let's see if tri matches are better than bimatches
There are no trimatches.

## Options from here:

1. Try to incorporate 4-grams

2. try a larger dataset, no removeSparse

3. Something more complex!

4. Ensure that trigrams are all from the same sentence??

### Of course, easiest one is to try larger dataset:
Just replace triTDM with triTDMlarge

Result: beer not even in top 10 now.
So perhaps more sparse elimination would help? I don't see how though...
and still no trimatches.
perhaps stick with remove sparse and add the other corpuses?

X # Get the frequency of each predFreq gram in our 'bag of words' freqTable
	unimatches<- uniFreqTable[uniFreqTable$gram==predFreq$gram,]

	uniFreqTable[uniFreqTable$gram == "beer",]
	uniFreqTable[uniFreqTable$gram == predFreq$gram,]
	#longer object length is not a multiple of shorter object length
	which(uniFreqTable$gram %in% predFreq$gram)

	uniPreds<-uniFreqTable[which(uniFreqTable$gram %in% predFreq$gram),]
	predFreq$gram # Is our vector...

	AH a problem is if some terms are NOT found in our uniFreq!

	OK I don't understand why my original syntax is giving me shit, I guess because they are not equal lenght, fine.

## SO my plan is now:
_Done in Model 3.3 `uniFreqTable`_ 1. Use bag of words to build a list of the most frequently used unigrams (words) 

*Done in Model 3.3 `predFreq`* 2. Use trigram model to find an array of possible predictions.

3. Build a mechanism which takes 

# Model 3.3

Not using sparse terms, I got a triTDM of 10721832 !
probably should reduce that...

saved my session, now ran:
	
	triTDM2<-removeSparseTerms(triTDM,0.01)

That obliterated it, so i'm trying now .0001

hmm seems to be getting worse... should i use bigger numbers?

0.99 ok now here we go:
731102 now.

let's do 0.98 i'd say.

277012

OK... 
let's try it.


#Model 3.2
My issue is that my trigram function perhaps cuts away too much leaving only 775 trigrams.
Try a different value on make sparse, lower than 0.50, I'll try 0.20
Not much better.
Probably should do this complete?

# Bag of Words ? Problem is there are no associations there! it can only tell us the probability of a word in GENERAL.

So... That is really not useful in terms of prediction EXCEPT to compare and choose the most likely of the trigram options perhaps.

# Model 3
? http://www.williamwebber.com/research/teaching/comp90042/2014s1/lect/l02.pdf
## NEXT STEPS:
1. should i try again where each line is a document? 
	As long as I am not depending on findAssocs, I suppose this is irrelevant. So the key issue will then be, *is my TrigramTokenizer accurate or is it meshing together sentences that bear no relation?*AND it makes it hard to use sparsity measures.  

	Question: is there an _ideal_ document length for NLP?

2. Thinking about machine learning... i suppose you are assigning frequencies and coming up with some kind of equation to predict the words, and possibly using k-folds / cross-validation. 

[Good Turing](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=86)
[Stanford nlp](https://class.coursera.org/nlp/lecture)

3. 

# Future study / forum

I wonder what the big speed up was. Was it just loading into a list?

### Fairly irrelevant for now but wondering:
1. Could I have broken up the file using only R not bash split?

# Model 2

Start over based on n-gram probabilities model, with just bigrams, and using small data first.

Break a sentence into bigrams, and find the probability of that sentence, fine, but then what?? [](]https://class.coursera.org/nlp/lecture/128)


# OK this is just unmanageable, 
I think I Need to go removing many terms from each file.

I feel like, similar to your problems with many tasks including songwriting, you flail about looking for shortcuts and easy answers instead of taking a DEEP BREATH and READING the hard documentation and doing the careful sober planning.

## Create a Pcorpus?

## bag of words model could be done with this? :
This would give us all the words in our prediction query sentence and their frequency... but actually that doesn't give us predictions.
inspect(TermDocumentMatrix(reuters, list(dictionary = c("prices", "crude", "oil"))))


#---#
# Export? (crashed)

dput(myTDM, file="myTDM.dput")

dump?

## Import
myTDM<-dget(myTDM, file="myTDM.dput")


## MUST UNDERSTAND:

1. Cross-validation / sampling / bootstrapping

2. *Markov Model / Markov Chain*

3. Smoothing by assigning non-zero probabilities?

4. multinomial or categorial distribution

5. (n-1) gram

6. Katz's Back-off model

7. Good-Turing frequency estimation

8. Use of Zipfian ?

-Can I find all 2- and 3-grams which include a word? not just search for a word...

9. Support vector machine

10. http://en.wikipedia.org/wiki/Latent_semantic_indexing

# STATUS at 4:20 11/10/14
We have a working model, 
Model1.scalingup4.list
Which does trigram and bigram detection.
We loaded in # 9270280 Terms, 2020000 Docs (lines) of Twitter data.
It took nearly 2 hours.


# QUESTION
Would having all as a document instead of each line being a document speed up?

# Reading in large files 

The error comes not with readLines but with the corpus work...
so really i just need to iterate I Think....
then combine...

## let's test first by combining two small TDMs and ensuring this works.

Ok works (see Combining Corpus Test.r)

## Now BASH is easiest way to split the file.

	split --lines=20000 en_US.twitter.txt twitter
outputs:
	twitteraa
	twitterab
etc.

[](https://www.gnu.org/software/coreutils/manual/html_node/split-invocation.html)

Downloaded the latest version of coreutils first in order to use the -d numeric option.

	split --numeric-suffixes -a 4 --lines=20000 en_US.twitter.txt twitter

```r
for (i in 1:118) {
i4<-formatC(i,width=4,flag=0)
fileName=paste0("twitter",i4,".txt")
varName = paste0("twit",i)
varName <- readLines(fileName)
}
```

# Model1 debugging
Using 20,000 lines of twitter, I get:
> gramCount<-rowSums(as.matrix(myTDM))
Error: cannot allocate vector of size 15.7 Gb
In addition: Warning messages:
1: In vector(typeof(x$v), nr * nc) :
	Reached total allocation of 12220Mb: see help(memory.size)

Solutions proposed:

- use library("Matrix")

Will not be soo simple...
Why do I need to do this?
can I get the counts another way?

- in tm package?

termFreq(myTDM)

dictionary?

- qdap package?

library(qdap)
wfm<-as.wfm(myTDM)

- can I just rowsums the TDM?

- slam package?
	
	X ph.DTM3 <- rollup(ph.DTM, 2, na.rm=TRUE, FUN = sum)

library(slam)
row_sums(myTDM)

*works!*

- http://stackoverflow.com/questions/18101047/list-of-word-frequencies-using-r
temp <- inspect(myTDM)
FreqMat <- data.frame(apply(temp, 1, sum))
FreqMat <- data.frame(ST = row.names(FreqMat), Freq = FreqMat[, 1])
FreqMat <- FreqMat[order(FreqMat$Freq, decreasing = T), ]
row.names(FreqMat) <- NULL
View(FreqMat)

- http://stackoverflow.com/questions/20004493/convert-simple-triplet-matrixslam-to-sparse-matrixmatrix-in-r



# Stem Completion?
See stemCompletion in tm package

# NEXT STEPS

1. Probably should work on fine-tuning the model before loading so much data? Not sure of this...

	-`proof of concept` - yes build a small corpus where you are trying to get a certain prediction to work first... How would this go?

```txt
"Talking heads are my favorite band"
"I hate the talking heads on tv like Wolf Blitzer"
"My favorite band is Aerosmith"
"Wolf Blitzer is not my favorite"
```

OK so that corpus should be able to predict that:
"Talking heads are my favorite" gives "band" ?

>You are looking for a (statistical) language model.

A statistical language model assigns a probability to a sequence of m words P(w_1,...,w_m) by means of a probability distribution...

So perhaps do it with only trigrams, 

So what i need to do is:

## CONCERNS:
-I'm not sure this is correct to eliminate "is". If eliminated from my corpus, of course it may as well be from input, but that is an issue (stopwords I think.)

```r

## CORPUS MUNGING ##

# 1. Corpus, transformations, and TDM Creation
#=============================================#
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone")
fileName="testData1.txt"
lineNews <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(lineNews, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
library(tm)
makeCorpus <- function(x) corpus <- {
	corpus<-Corpus(VectorSource(x))
	corpus <- tm_map(corpus, stripWhitespace)
	corpus <- tm_map(corpus, content_transformer(tolower))
	corpus <- tm_map(corpus, removeWords, stopwords("english"))
	corpus <- tm_map(corpus, stemDocument)
	corpus<- tm_map(corpus,removePunctuation)
	corpus<- tm_map(corpus,removeNumbers)
}
corpus<-makeCorpus(lineNews)

# Build TDM with tri-grams.
library(rJava) # Is this really needed?
library(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

myTDM <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))

# 2. Isolate bigrams and unigrams within trigrams 
#=============================================#

# Get total frequency in corpus of each trigram
gramCount<-rowSums(as.matrix(myTDM))

# Create dataframe frequency table
freqTable <- data.frame(gram=names(gramCount),count=gramCount,stringsAsFactors=FALSE)

# Split corpus trigrams up to words
words <- strsplit(freqTable$gram," ")

# Set first two words as an attribute, the trigram prediction query pair of words
freqTable$triquery <- sapply(words,FUN=function(x) paste(x[1],x[2]))

# Set each word of trigram as an attribute for future use
freqTable$one <- sapply(words,FUN=function(x) paste(x[1]))
freqTable$two <- sapply(words,FUN=function(x) paste(x[2]))
freqTable$three <- sapply(words,FUN=function(x) paste(x[3]))

# 3. INPUT MUNGING
#=============================================#
## INPUT MUNGING ##
# i. Take an input:
input<-"my favorite band is talking"

# ii. Perform Transformations.
input<-makeCorpus(input)
input<-as.character(input[[1]][1])

# iii. Reduce to last two words
input<-unlist(strsplit(input,"\\s+"))
two<-length(input)
one<-two-1
# iv. set querying bigrams and unigrams we will search for in trigrams and bigrams respectively
bigram<-paste(input[one],input[two])
unigram<-paste(input[two])

# 4. FIND PREDICTION MATCHES
#=============================================#

#TRIGRAMS:

# Find trigrams where first two words match and put in matches list
trimatches <- freqTable[freqTable$triquery == bigram,]
bimatch1 <- freqTable[freqTable$one == unigram,]
bimatch2 <- freqTable[freqTable$two == unigram,]

# Print out results:
for (i in 1:nrow(trimatches)){print(paste("Trigram prediction ",i,": ",trimatches$three[i]))}
for (i in 1:nrow(bimatch1)){print(paste("Bigram prediction ",i,": ",bimatch1$two[i]))}
k=nrow(bimatch1)
for (i in 1:nrow(bimatch1)){
k=k+1
print(paste("Bigram prediction ",k,": ",bimatch1$two[i]))}

# Next: Instead of printing, put these results in a frequency table and rank them as such.
matches<-c(trimatches$three,bimatch1$two,bimatch2$one)
matchCorpus<-makeCorpus(matches) # (Corpus(VectorSource(matches)) caused very similar terms to be considered separately like singular and plural)
matchTDM<-TermDocumentMatrix(matchCorpus)

# Get total frequency in prediction corpus of each prediction
predCount<-rowSums(as.matrix(matchTDM))

# Create dataframe frequency table
predFreq <- data.frame(gram=names(predCount),count=predCount,stringsAsFactors=FALSE)
predFreq<-predFreq[order(-predFreq$count),]
```


# TO DO NEXT:

## Unstem? most frequent versions of that stem?

CUT:

	for (i in trimatches) {paste("Trigram prediction ",i,": ",trimatches$three)}
	paste("Bigram predictions: ", bimatch1$two, bimatch2$one)

	# Set trigram predicted word as an attribute of our data.frame freqTable
	freqTable$tripredicted <- sapply(words,FUN=function(x) x[3])

	# A unigram query word may be either the first or second word of the trigram, depending on where in the trigram the last word of our input is. 

	# Set the predicted by bigram as an attribute of our data.frame freqTable
	freqTable$bipredicted <- sapply(words, FUN=function(x) {if (x[1] == unigram) {paste(x[2])}else if(x[2] == unigram) {paste(x[3])}else {paste(NA)}})



	# If matches were found, print these, otherwise move on to bigram matches
	if (nrow(trimatches)>0) {paste("trigram matches:", trimatches)}
	bimatches<- freqTable[freqTable$bipredicted != "NA",] 
	if (nrow(bimatches)>0) {paste("bigram matches:", bimatches$gram)}

	---
	#uniTest<- function(x) {if (x[1] == unigram) {paste(x[2])}else if(x[2] == unigram) {paste(x[3])}else {paste(NA)}}

	#freqTable$uniquery <- sapply(words, FUN=uniTest(x))


	# uniTable<-freqTable[freqTable$uniquery == unigram,]
	#freqTable$uniquery <- sapply(words,FUN=function(x) paste(x[2]))

#*OK at this point I have done the lookup to query earlier, before making the freqTable with uniquery, why not also with biquery?* 
* Perhaps because building the freqtable again will be laborious?

# Next: The lookup, and, Good-Turing?
*For the unigram it will not always be the second word of the trigram, it will be whichever matches the last word of the query bigram.*

```r
# Find matches to query words
matches <- freqTable[freqTable$biquery == bigram,]
# test if matches
if nrows(matches>0) {
	print("trigram matches:")
	print(matches$predicted)
	# do something
}
else {
	matches<- freqTable[freqTable$uniquery != NA,]
	print("bigram matches:")
	print(matches$predicted)
}
# This will work if we can also use the corpus query words as bigrams and match to the last word in our bigram query.

```

# The following procedure comes from http://rstudio-pubs-static.s3.amazonaws.com/33754_4d463ac84bd24721bb9fe6a707ef6236.html and it uses a dataframe which finds *probabilities* of each trigram, not relying on findAssoc.

```r
# get the subset of the trigram data table with a matching bigram start
	swf_T <- wf_T[wf_T$start == bigram,]
# where wf_T is a data.frame in which:
# columns are: words and count
# rows are: names(freq_T), freq_T
# where freq_T is a table created by rowSums(as.matrix(trainTDM_T))
# AND where wf_T$start = sapply(triL,FUN=function(x) paste(x[1],x[2]))
```


My question is... what was the point of TDM? well I guess that compiled it to a per document thing. But now, I don't care about the individual documents anymore so I just am summing things up to get probabilities.

## So take my small test TDM:

> inspect(myTDM)
<<TermDocumentMatrix (terms: 29, documents: 4)>>
Non-/sparse entries: 38/78
Sparsity           : 67%
Maximal term length: 22
Weighting          : term frequency (tf)

												Docs
Terms                    1 2 3 4
	aerosmith              0 0 1 0
	band                   1 0 1 0
	band aerosmith         0 0 1 0
	blitzer                0 1 0 1
	blitzer favorite       0 0 0 1
	favorit                1 0 1 0
	favorit band           1 0 1 0
	favorit band aerosmith 0 0 1 0
	favorite               0 0 0 1
	hate                   0 1 0 0
	hate talk              0 1 0 0
	hate talk head         0 1 0 0
	head                   1 1 0 0
	head favorit           1 0 0 0
	head favorit band      1 0 0 0
	head tv                0 1 0 0
	head tv like           0 1 0 0
	like                   0 1 0 0
	like wolf              0 1 0 0
	like wolf blitzer      0 1 0 0
	talk                   1 1 0 0
	talk head              1 1 0 0
	talk head favorit      1 0 0 0
	talk head tv           0 1 0 0
	tv like                0 1 0 0
	tv like wolf           0 1 0 0
	wolf                   0 1 0 1
	wolf blitzer           0 1 0 1
	wolf blitzer favorite  0 0 0 1

no, lets go back and do just trigrams (edit TriGramTokenizer function)
OK:
> inspect(myTDM)
<<TermDocumentMatrix (terms: 9, documents: 4)>>
Non-/sparse entries: 9/27
Sparsity           : 75%
Maximal term length: 22
Weighting          : term frequency (tf)

												Docs
Terms                    1 2 3 4
	favorit band aerosmith 0 0 1 0
	hate talk head         0 1 0 0
	head favorit band      1 0 0 0
	head tv like           0 1 0 0
	like wolf blitzer      0 1 0 0
	talk head favorit      1 0 0 0
	talk head tv           0 1 0 0
	tv like wolf           0 1 0 0
	wolf blitzer favorite  0 0 0 1

```r

#get total count of each trigram
gramCount<-rowSums(as.matrix(myTDM))

#Create dataframe frequency table
freqTable <- data.frame(gram=names(gramCount),count=gramCount,stringsAsFactors=FALSE)

#split the trigram up to define the first two 'query' words
words <- strsplit(freqTable$gram," ")

#set query words as an attribute
freqTable$query <- sapply(words,FUN=function(x) paste(x[1],x[2]))

# Set predicted word as an attribute
freqTable$predicted <- sapply(words,FUN=function(x) x[3])
```



## Speed Issues
http://www.r-bloggers.com/faster-higher-stonger-a-guide-to-speeding-up-r-code-for-busy-people/

-Run gc() throughout?

-`tm_combine` Combine Corpora, Documents, Term-Document Matrices, and Term
Frequency Vectors

So, I could load portions of the files then combine.

```r
c(myTDM,myTDM2)
```

## QUIZ 2:

>Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order. This is good enough to finish quizz ( I think). But the main disadvantage is the low speed. It takes about one minute to search.

# MODEL:
## Assigning weights/points:

See tm package documentation.pdf:
`tm_term_score` - compute a score based on the number of matching terms.
`weightSMART` SMART Weightings

# Modeling problem.
[Main forum](https://class.coursera.org/dsscapstone-002/forum/list?forum_id=10010)

## [Stanford NLP Lectures](https://class.coursera.org/nlp/lecture)

What I'm learning:
my original corpus might be okay, but I need a way to call out by a number each word in a sentence.
Then, add up all the probabilities of unigrams, 
but also check the n-1

### -Maybe I should examine the code of tm package or rtexttools
[](https://code.google.com/p/rtexttools/source/browse/RTextTools/R/create_matrix.R?r=02bd8e07c0df54e44aff83a05dff1cc74d82c4de)
Looks like it's just a wrapper for tm anyway!

[](http://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package?lq=1)

```r
library(tm); library(tau);

tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))
```

### Intro to N-grams

LM = language modeling

What we are doing is assigning probabilities to different sentences, words.

#### Conditional Probability definition
P(A|B) = P(A,B)/P(B)

#### Chain rule of conditional probability
P(x1,x2,x3,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,...,xn-1)

#### Markov Assumption
P("the" | "its water is so transparent that") ≈ P(the|that) or maybe (the|"transparent that")

#### Unigram model
Since it simply assigns probability to each word, the nonsense sentence "the the the the" would be highest probability.

#### N-grams 

language has long-distance dependencies, the verb can come very far after the subject which really is an important predictor "the computer which I had just placed on the fifth floor *crashed*"

Yet, 2-5-gram models will do pretty well anyway.

### Bigram model

P(wi|wi-1) = count(wi-1,wi)/count(wi-1)

#### Types of knowledge that cause bigram probabilities
World knowledge (chinese food more popular then english food)
Grammar knowledge ("want" often followed by infinitive like "to")

Structural - something really is that probability
Contingent - result of the particular corpus sample

#### Practical Issues

adding log probabilities is faster than multiplying regular probabilities.

Toolkits:
SRILM
Google N-gram release

### Evaluation and Perplexity

training set, test set, evaluation metric
A task is the best evaluation for comparing multiple models:
	-spelling corrector, speech recognizer, MT system
Accuracy measure	
	-how many misspelled words corrected properly
	-how many words translated correctly

Above is extrinsic or in-vivo evaluation. Problem is, time-consuming slow.

#### Intrinsic evaluation - perplexity

unless test data very similar to training data, not very useful, still need to do extrinsic.

#### The Shannon Game - how well can we predict the next word.



## Most helpful thread: [](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=93#post-461)

-Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order.

-The Stanford NLP video lectures were very helpful on tokenisation and creating ngrams. Think I'll take a look at the modelling ones.

-I just went through the Language Modeling lectures from the Natural Language Processing course by Dan Jurafsky, Christopher Manning. They are VERY helpful. I suggest watching the entire week's worth.  

## This? [Build a search engine in 20 minutes or less](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)
## Trying rtexttools instead of tm to make TDM
```r
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="en_US.twitter.txt"
lineNews <- readLines(fileName, n=100000)

### NOTE: I SHOULD REMOVE PROFANITY AS WELL SOMEWHERE ###

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(lineNews, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

require(RTextTools)
myTDM <- create_matrix(cleanData3, language="english", 
										removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
										removePunctuation=TRUE, removeSparseTerms=0.60, 
										removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#creates a matrix where terms are columns, documents are rows.
colnames(myTDM) #lists all terms
# ok above results in only 32 words!
myTDM <- create_matrix(cleanData3, language="english", 
										removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
										removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)

# Could problem be that I did not load tm or tau?
# okay, that worked and produced 48399 columns

## VIEW ##
options(max.print=100000)
sink("myTDM.txt")
findFreqTerms(x = myTDM, lowfreq=1, highfreq=Inf)
sink()

# so it seems i have only unigrams, and that the reason may be that words have been mashed together for some reason. I will try again without all the other options.

myTDM <- create_matrix(cleanData3, language="english", 
										removeNumbers=FALSE, stemWords=FALSE, ngramLength=c(2),  
										removePunctuation=FALSE, removeStopwords=FALSE, stripWhitespace=FALSE, toLower=TRUE)
# Error in FUN(X[[2L]], ...) : non-character argument

debug(create_matrix)
undebug(create_matrix)
#OR debugonce(create_matrix)

capture.output(create_matrix(cleanData3, language="english", 
										removeNumbers=FALSE, stemWords=FALSE, ngramLength=c(2),  
										removePunctuation=FALSE, removeStopwords=FALSE, stripWhitespace=FALSE, toLower=TRUE), file='source_create_matrix.r')
```

```r
tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))
```
This creates the same error, therefore, the problem is within this, since create_matrix calls this.

```r
tokenize_ngrams("This is the first document.")
# [1] "is the first"       "the first document" "this is the"
```
so that works... 
 so the error is in the control= or tokenize= thing.
SO, the easiest way for me to work this i guess would be to simply tokenize ahead of time? 
I could create separate matrix for bigrams. But how to keep documents together.

# LET"S READ THE DOCS FOR tm CONTROL=
```r
matrix <- TermDocumentMatrix(corpus)
```
So the problem seems to be that a custom function for tokenize... well i guess it is allowed but perhaps this one doesn't do it right.

```r
library(rJava)
library(RWeka)
library(tm)

bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = bigramTokenizer))
```
Woot! that worked...

```r
library(RTextTools)
TDM2 <- create_matrix(corpus, language="english", 
										removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
										removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#Warning message:
#In if (ngramLength > 1) { :
	#the condition has length > 1 and only the first element will be used
```
So, that way does not work.

```r
library(rJava)
library(RWeka)
library(tm)

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
```
#*Sweet! That seems to do it.*#

#--#
'added today' %in% Terms(myTDM)
FALSE
findAssocs(myTDM,'know',0)

options(max.print=100000)
sink("myTDM.txt")		
inspect(myTDM)
sink()

```


# "case of" is not in the dataset en_US.news.txt

```r
'case of' %in% Terms(txtTdmBi)
FALSE
'added today' %in% Terms(txtTdmBi)
TRUE
findAssocs(txtTdmBi2, "added today", 0)

rowSums(as.matrix(txtTdmBi))
```

#OK let's see the top bigrams;

```r
findFreqTerms(x = txtTdmBi2, lowfreq=158, highfreq=Inf)
findAssocs(txtTdmBi2, "barack obama", 0.1)
```

# SPEED ISSUES

```r
library(gamlr)
findAssocsBig <- function(u, term, corlimit){
	suppressWarnings(x.cor <-  gamlr:corr(t(u[ !u$dimnames$Terms == term, ]),        
																				 as.matrix(t(u[  u$dimnames$Terms == term, ]))  ))  
	x <- sort(round(x.cor[(x.cor[, term] > corlimit), ], 2), decreasing = TRUE)
	return(x)
}
findAssocsBig(txtTdmBi, "added today", 0.01)
#Error in get(name, envir = asNamespace(pkg), inherits = FALSE) : 
#  object 'corr' not found
```
# large corpus finding nothing...

```r
findFreqTerms(x = tdm, lowfreq=1758, highfreq=Inf)
 [1] "also"    "back"    "call"    "can"     "citi"    "come"   
 [7] "compani" "counti"  "day"     "even"    "first"   "game"   
[13] "get"     "good"    "help"    "high"    "home"    "includ" 
[19] "just"    "know"    "last"    "like"    "look"    "made"   
[25] "make"    "mani"    "may"     "million" "month"   "much"   
[31] "need"    "new"     "now"     "one"     "open"    "peopl"  
[37] "percent" "plan"    "play"    "point"   "polic"   "public" 
[43] "report"  "right"   "run"     "said"    "say"     "school" 
[49] "season"  "see"     "show"    "sinc"    "start"   "state"  
[55] "still"   "take"    "team"    "think"   "three"   "time"   
[61] "tri"     "two"     "use"     "want"    "way"     "week"   
[67] "well"    "will"    "work"    "year"

findAssocs(tdm,"first",corlimit=0.5)
# how can this be 0... too many words?
findAssocs(tdm,"first",corlimit=0.95)
# nope
findAssocs(tdm,"first",corlimit=0.01)
# a ha, ok it's just that i have 77,000 documents and nothing shows up more than maybe 1758 times...
findAssocs(tdm,"first",corlimit=0.02)

#let's try this with "case"
findAssocs(tdm,"case",corlimit=0.03)
# everything is legal related. looks like bigrams will be needed.

```

# bigram
RWeka package
bigram tokenizer...

```r
#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
```

# Idea
Load the test sentence as a TDM.
Use findAssoc to get words?

```r
#Set working directory
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="testData.txt"
testData <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(testData, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
require(tm)
corpus <- Corpus(VectorSource(cleanData3))

## TOKENIZATION ## 

# REMOVE WHITESPACE:
corpus1 <- tm_map(corpus, stripWhitespace)
inspect(corpus1) #don't see a big difference

# LOWERCASE:
corpus2 <- tm_map(corpus1, content_transformer(tolower))
inspect(corpus2) #works

# REMOVE STOPWORDS
corpus3 <- tm_map(corpus2, removeWords, stopwords("english"))
inspect(corpus3) # ok the has been removed...

# STEMMING
corpus4 <- tm_map(corpus3, stemDocument)
inspect(corpus4) # Looks stemmed.

# REMOVE PUNCTUATION
corpus5<- tm_map(corpus4,removePunctuation)

# REMOVE NUMBERS
corpus6<- tm_map(corpus5,removeNumbers)

## END TOKENIZATION ##

## TOKEN ANALYSIS ##

# MAKE TERM DOCUMENT MATRIX (TDM) - a matrix of frequency counts for each word used in the corpus.
tdm<- TermDocumentMatrix(corpus6)
dtm<- DocumentTermMatrix(corpus6)
dtm

```

# Implement "Bag of Words" model

I need to weight the words.


[](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)

# SOME COMMANDS:

```r
findFreqTerms(x = tdm, lowfreq=58, highfreq=Inf)
findAssocs(x=tdm,term="also",corlimit=0.1)

# 1. take a sentence and iterate through the words

# 2. Look up associated words 

# 3. Give a score to each association with higher score to the words near the blank. 

## POSSIBILITIES:

# 1. REMOVE SPARSE TERMS
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
# [1] 18651   182
dim(TDM.common)
# [1]  71 182
```

# Quiz 2
## Question 1
The guy in front of me just bought a pound of bacon, a bouquet, and a case of
soda
pretzels
cheese
beer

# Vocab
*orthogonal* - right-angle but in statistics means not related.
*dot product* - an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.
*Euclidean space* - encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces
