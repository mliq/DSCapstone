Post-Tokenization
====
? http://www.williamwebber.com/research/teaching/comp90042/2014s1/lect/l02.pdf
# Model 3
## NEXT STEPS:
1. should i try again where each line is a document? 
	As long as I am not depending on findAssocs, I suppose this is irrelevant. So the key issue will then be, *is my TrigramTokenizer accurate or is it meshing together sentences that bear no relation?*AND it makes it hard to use sparsity measures.  

	Question: is there an _ideal_ document length for NLP?

2. Thinking about machine learning... i suppose you are assigning frequencies and coming up with some kind of equation to predict the words, and possibly using k-folds / cross-validation. 

[Good Turing](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=86)
[Stanford nlp](https://class.coursera.org/nlp/lecture)

3. 

### Fairly irrelevant for now but wondering:
1. Could I have broken up the file using only R not bash split?

# Model 2

Start over based on n-gram probabilities model, with just bigrams, and using small data first.

Break a sentence into bigrams, and find the probability of that sentence, fine, but then what?? [](]https://class.coursera.org/nlp/lecture/128)


# OK this is just unmanageable, 
I think I Need to go removing many terms from each file.

I feel like, similar to your problems with many tasks including songwriting, you flail about looking for shortcuts and easy answers instead of taking a DEEP BREATH and READING the hard documentation and doing the careful sober planning.

## Create a Pcorpus?

## bag of words model could be done with this? :
This would give us all the words in our prediction query sentence and their frequency... but actually that doesn't give us predictions.
inspect(TermDocumentMatrix(reuters, list(dictionary = c("prices", "crude", "oil"))))


#---#
# Export? (crashed)

dput(myTDM, file="myTDM.dput")

dump?

## Import
myTDM<-dget(myTDM, file="myTDM.dput")


## MUST UNDERSTAND:

1. Cross-validation / sampling / bootstrapping

2. *Markov Model / Markov Chain*

3. Smoothing by assigning non-zero probabilities?

4. multinomial or categorial distribution

5. (n-1) gram

6. Katz's Back-off model

7. Good-Turing frequency estimation

8. Use of Zipfian ?

-Can I find all 2- and 3-grams which include a word? not just search for a word...

9. Support vector machine

10. http://en.wikipedia.org/wiki/Latent_semantic_indexing

# STATUS at 4:20 11/10/14
We have a working model, 
Model1.scalingup4.list
Which does trigram and bigram detection.
We loaded in # 9270280 Terms, 2020000 Docs (lines) of Twitter data.
It took nearly 2 hours.


# QUESTION
Would having all as a document instead of each line being a document speed up?

# Reading in large files 

The error comes not with readLines but with the corpus work...
so really i just need to iterate I Think....
then combine...

## let's test first by combining two small TDMs and ensuring this works.

Ok works (see Combining Corpus Test.r)

## Now BASH is easiest way to split the file.

	split --lines=20000 en_US.twitter.txt twitter
outputs:
	twitteraa
	twitterab
etc.

[](https://www.gnu.org/software/coreutils/manual/html_node/split-invocation.html)

Downloaded the latest version of coreutils first in order to use the -d numeric option.

	split --numeric-suffixes -a 4 --lines=20000 en_US.twitter.txt twitter

```r
for (i in 1:118) {
i4<-formatC(i,width=4,flag=0)
fileName=paste0("twitter",i4,".txt")
varName = paste0("twit",i)
varName <- readLines(fileName)
}
```

# Model1 debugging
Using 20,000 lines of twitter, I get:
> gramCount<-rowSums(as.matrix(myTDM))
Error: cannot allocate vector of size 15.7 Gb
In addition: Warning messages:
1: In vector(typeof(x$v), nr * nc) :
  Reached total allocation of 12220Mb: see help(memory.size)

Solutions proposed:

- use library("Matrix")

Will not be soo simple...
Why do I need to do this?
can I get the counts another way?

- in tm package?

termFreq(myTDM)

dictionary?

- qdap package?

library(qdap)
wfm<-as.wfm(myTDM)

- can I just rowsums the TDM?

- slam package?
	
	X ph.DTM3 <- rollup(ph.DTM, 2, na.rm=TRUE, FUN = sum)

library(slam)
row_sums(myTDM)

*works!*

- http://stackoverflow.com/questions/18101047/list-of-word-frequencies-using-r
temp <- inspect(myTDM)
FreqMat <- data.frame(apply(temp, 1, sum))
FreqMat <- data.frame(ST = row.names(FreqMat), Freq = FreqMat[, 1])
FreqMat <- FreqMat[order(FreqMat$Freq, decreasing = T), ]
row.names(FreqMat) <- NULL
View(FreqMat)

- http://stackoverflow.com/questions/20004493/convert-simple-triplet-matrixslam-to-sparse-matrixmatrix-in-r



# Stem Completion?
See stemCompletion in tm package

# NEXT STEPS

1. Probably should work on fine-tuning the model before loading so much data? Not sure of this...

	-`proof of concept` - yes build a small corpus where you are trying to get a certain prediction to work first... How would this go?

```txt
"Talking heads are my favorite band"
"I hate the talking heads on tv like Wolf Blitzer"
"My favorite band is Aerosmith"
"Wolf Blitzer is not my favorite"
```

OK so that corpus should be able to predict that:
"Talking heads are my favorite" gives "band" ?

>You are looking for a (statistical) language model.

A statistical language model assigns a probability to a sequence of m words P(w_1,...,w_m) by means of a probability distribution...

So perhaps do it with only trigrams, 

So what i need to do is:

## CONCERNS:
-I'm not sure this is correct to eliminate "is". If eliminated from my corpus, of course it may as well be from input, but that is an issue (stopwords I think.)

```r

## CORPUS MUNGING ##

# 1. Corpus, transformations, and TDM Creation
#=============================================#
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone")
fileName="testData1.txt"
lineNews <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(lineNews, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
library(tm)
makeCorpus <- function(x) corpus <- {
	corpus<-Corpus(VectorSource(x))
	corpus <- tm_map(corpus, stripWhitespace)
	corpus <- tm_map(corpus, content_transformer(tolower))
	corpus <- tm_map(corpus, removeWords, stopwords("english"))
	corpus <- tm_map(corpus, stemDocument)
	corpus<- tm_map(corpus,removePunctuation)
	corpus<- tm_map(corpus,removeNumbers)
}
corpus<-makeCorpus(lineNews)

# Build TDM with tri-grams.
library(rJava) # Is this really needed?
library(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

myTDM <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))

# 2. Isolate bigrams and unigrams within trigrams 
#=============================================#

# Get total frequency in corpus of each trigram
gramCount<-rowSums(as.matrix(myTDM))

# Create dataframe frequency table
freqTable <- data.frame(gram=names(gramCount),count=gramCount,stringsAsFactors=FALSE)

# Split corpus trigrams up to words
words <- strsplit(freqTable$gram," ")

# Set first two words as an attribute, the trigram prediction query pair of words
freqTable$triquery <- sapply(words,FUN=function(x) paste(x[1],x[2]))

# Set each word of trigram as an attribute for future use
freqTable$one <- sapply(words,FUN=function(x) paste(x[1]))
freqTable$two <- sapply(words,FUN=function(x) paste(x[2]))
freqTable$three <- sapply(words,FUN=function(x) paste(x[3]))

# 3. INPUT MUNGING
#=============================================#
## INPUT MUNGING ##
# i. Take an input:
input<-"my favorite band is talking"

# ii. Perform Transformations.
input<-makeCorpus(input)
input<-as.character(input[[1]][1])

# iii. Reduce to last two words
input<-unlist(strsplit(input,"\\s+"))
two<-length(input)
one<-two-1
# iv. set querying bigrams and unigrams we will search for in trigrams and bigrams respectively
bigram<-paste(input[one],input[two])
unigram<-paste(input[two])

# 4. FIND PREDICTION MATCHES
#=============================================#

#TRIGRAMS:

# Find trigrams where first two words match and put in matches list
trimatches <- freqTable[freqTable$triquery == bigram,]
bimatch1 <- freqTable[freqTable$one == unigram,]
bimatch2 <- freqTable[freqTable$two == unigram,]

# Print out results:
for (i in 1:nrow(trimatches)){print(paste("Trigram prediction ",i,": ",trimatches$three[i]))}
for (i in 1:nrow(bimatch1)){print(paste("Bigram prediction ",i,": ",bimatch1$two[i]))}
k=nrow(bimatch1)
for (i in 1:nrow(bimatch1)){
k=k+1
print(paste("Bigram prediction ",k,": ",bimatch1$two[i]))}

# Next: Instead of printing, put these results in a frequency table and rank them as such.
matches<-c(trimatches$three,bimatch1$two,bimatch2$one)
matchCorpus<-makeCorpus(matches) # (Corpus(VectorSource(matches)) caused very similar terms to be considered separately like singular and plural)
matchTDM<-TermDocumentMatrix(matchCorpus)

# Get total frequency in prediction corpus of each prediction
predCount<-rowSums(as.matrix(matchTDM))

# Create dataframe frequency table
predFreq <- data.frame(gram=names(predCount),count=predCount,stringsAsFactors=FALSE)
predFreq<-predFreq[order(-predFreq$count),]
```


# TO DO NEXT:

## Unstem? most frequent versions of that stem?

CUT:

	for (i in trimatches) {paste("Trigram prediction ",i,": ",trimatches$three)}
	paste("Bigram predictions: ", bimatch1$two, bimatch2$one)

	# Set trigram predicted word as an attribute of our data.frame freqTable
	freqTable$tripredicted <- sapply(words,FUN=function(x) x[3])

	# A unigram query word may be either the first or second word of the trigram, depending on where in the trigram the last word of our input is. 

	# Set the predicted by bigram as an attribute of our data.frame freqTable
	freqTable$bipredicted <- sapply(words, FUN=function(x) {if (x[1] == unigram) {paste(x[2])}else if(x[2] == unigram) {paste(x[3])}else {paste(NA)}})



	# If matches were found, print these, otherwise move on to bigram matches
	if (nrow(trimatches)>0) {paste("trigram matches:", trimatches)}
	bimatches<- freqTable[freqTable$bipredicted != "NA",] 
	if (nrow(bimatches)>0) {paste("bigram matches:", bimatches$gram)}

	---
	#uniTest<- function(x) {if (x[1] == unigram) {paste(x[2])}else if(x[2] == unigram) {paste(x[3])}else {paste(NA)}}

	#freqTable$uniquery <- sapply(words, FUN=uniTest(x))


	# uniTable<-freqTable[freqTable$uniquery == unigram,]
	#freqTable$uniquery <- sapply(words,FUN=function(x) paste(x[2]))

#*OK at this point I have done the lookup to query earlier, before making the freqTable with uniquery, why not also with biquery?* 
* Perhaps because building the freqtable again will be laborious?

# Next: The lookup, and, Good-Turing?
*For the unigram it will not always be the second word of the trigram, it will be whichever matches the last word of the query bigram.*

```r
# Find matches to query words
matches <- freqTable[freqTable$biquery == bigram,]
# test if matches
if nrows(matches>0) {
	print("trigram matches:")
	print(matches$predicted)
	# do something
}
else {
	matches<- freqTable[freqTable$uniquery != NA,]
	print("bigram matches:")
	print(matches$predicted)
}
# This will work if we can also use the corpus query words as bigrams and match to the last word in our bigram query.

```

# The following procedure comes from http://rstudio-pubs-static.s3.amazonaws.com/33754_4d463ac84bd24721bb9fe6a707ef6236.html and it uses a dataframe which finds *probabilities* of each trigram, not relying on findAssoc.

```r
# get the subset of the trigram data table with a matching bigram start
  swf_T <- wf_T[wf_T$start == bigram,]
# where wf_T is a data.frame in which:
# columns are: words and count
# rows are: names(freq_T), freq_T
# where freq_T is a table created by rowSums(as.matrix(trainTDM_T))
# AND where wf_T$start = sapply(triL,FUN=function(x) paste(x[1],x[2]))
```


My question is... what was the point of TDM? well I guess that compiled it to a per document thing. But now, I don't care about the individual documents anymore so I just am summing things up to get probabilities.

## So take my small test TDM:

> inspect(myTDM)
<<TermDocumentMatrix (terms: 29, documents: 4)>>
Non-/sparse entries: 38/78
Sparsity           : 67%
Maximal term length: 22
Weighting          : term frequency (tf)

                        Docs
Terms                    1 2 3 4
  aerosmith              0 0 1 0
  band                   1 0 1 0
  band aerosmith         0 0 1 0
  blitzer                0 1 0 1
  blitzer favorite       0 0 0 1
  favorit                1 0 1 0
  favorit band           1 0 1 0
  favorit band aerosmith 0 0 1 0
  favorite               0 0 0 1
  hate                   0 1 0 0
  hate talk              0 1 0 0
  hate talk head         0 1 0 0
  head                   1 1 0 0
  head favorit           1 0 0 0
  head favorit band      1 0 0 0
  head tv                0 1 0 0
  head tv like           0 1 0 0
  like                   0 1 0 0
  like wolf              0 1 0 0
  like wolf blitzer      0 1 0 0
  talk                   1 1 0 0
  talk head              1 1 0 0
  talk head favorit      1 0 0 0
  talk head tv           0 1 0 0
  tv like                0 1 0 0
  tv like wolf           0 1 0 0
  wolf                   0 1 0 1
  wolf blitzer           0 1 0 1
  wolf blitzer favorite  0 0 0 1

no, lets go back and do just trigrams (edit TriGramTokenizer function)
OK:
> inspect(myTDM)
<<TermDocumentMatrix (terms: 9, documents: 4)>>
Non-/sparse entries: 9/27
Sparsity           : 75%
Maximal term length: 22
Weighting          : term frequency (tf)

                        Docs
Terms                    1 2 3 4
  favorit band aerosmith 0 0 1 0
  hate talk head         0 1 0 0
  head favorit band      1 0 0 0
  head tv like           0 1 0 0
  like wolf blitzer      0 1 0 0
  talk head favorit      1 0 0 0
  talk head tv           0 1 0 0
  tv like wolf           0 1 0 0
  wolf blitzer favorite  0 0 0 1

```r

#get total count of each trigram
gramCount<-rowSums(as.matrix(myTDM))

#Create dataframe frequency table
freqTable <- data.frame(gram=names(gramCount),count=gramCount,stringsAsFactors=FALSE)

#split the trigram up to define the first two 'query' words
words <- strsplit(freqTable$gram," ")

#set query words as an attribute
freqTable$query <- sapply(words,FUN=function(x) paste(x[1],x[2]))

# Set predicted word as an attribute
freqTable$predicted <- sapply(words,FUN=function(x) x[3])
```



## Speed Issues
http://www.r-bloggers.com/faster-higher-stonger-a-guide-to-speeding-up-r-code-for-busy-people/

-Run gc() throughout?

-`tm_combine` Combine Corpora, Documents, Term-Document Matrices, and Term
Frequency Vectors

So, I could load portions of the files then combine.

```r
c(myTDM,myTDM2)
```

## QUIZ 2:

>Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order. This is good enough to finish quizz ( I think). But the main disadvantage is the low speed. It takes about one minute to search.

# MODEL:
## Assigning weights/points:

See tm package documentation.pdf:
`tm_term_score` - compute a score based on the number of matching terms.
`weightSMART` SMART Weightings

# Modeling problem.
[Main forum](https://class.coursera.org/dsscapstone-002/forum/list?forum_id=10010)

## [Stanford NLP Lectures](https://class.coursera.org/nlp/lecture)

What I'm learning:
my original corpus might be okay, but I need a way to call out by a number each word in a sentence.
Then, add up all the probabilities of unigrams, 
but also check the n-1

### -Maybe I should examine the code of tm package or rtexttools
[](https://code.google.com/p/rtexttools/source/browse/RTextTools/R/create_matrix.R?r=02bd8e07c0df54e44aff83a05dff1cc74d82c4de)
Looks like it's just a wrapper for tm anyway!

[](http://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package?lq=1)

```r
library(tm); library(tau);

tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))
```

### Intro to N-grams

LM = language modeling

What we are doing is assigning probabilities to different sentences, words.

#### Conditional Probability definition
P(A|B) = P(A,B)/P(B)

#### Chain rule of conditional probability
P(x1,x2,x3,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,...,xn-1)

#### Markov Assumption
P("the" | "its water is so transparent that") â‰ˆ P(the|that) or maybe (the|"transparent that")

#### Unigram model
Since it simply assigns probability to each word, the nonsense sentence "the the the the" would be highest probability.

#### N-grams 

language has long-distance dependencies, the verb can come very far after the subject which really is an important predictor "the computer which I had just placed on the fifth floor *crashed*"

Yet, 2-5-gram models will do pretty well anyway.

### Bigram model

P(wi|wi-1) = count(wi-1,wi)/count(wi-1)

#### Types of knowledge that cause bigram probabilities
World knowledge (chinese food more popular then english food)
Grammar knowledge ("want" often followed by infinitive like "to")

Structural - something really is that probability
Contingent - result of the particular corpus sample

#### Practical Issues

adding log probabilities is faster than multiplying regular probabilities.

Toolkits:
SRILM
Google N-gram release

### Evaluation and Perplexity

training set, test set, evaluation metric
A task is the best evaluation for comparing multiple models:
	-spelling corrector, speech recognizer, MT system
Accuracy measure	
	-how many misspelled words corrected properly
	-how many words translated correctly

Above is extrinsic or in-vivo evaluation. Problem is, time-consuming slow.

#### Intrinsic evaluation - perplexity

unless test data very similar to training data, not very useful, still need to do extrinsic.

#### The Shannon Game - how well can we predict the next word.



## Most helpful thread: [](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=93#post-461)

-Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order.

-The Stanford NLP video lectures were very helpful on tokenisation and creating ngrams. Think I'll take a look at the modelling ones.

-I just went through the Language Modeling lectures from the Natural Language Processing course by Dan Jurafsky, Christopher Manning. They are VERY helpful. I suggest watching the entire week's worth.  

## This? [Build a search engine in 20 minutes or less](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)
## Trying rtexttools instead of tm to make TDM
```r
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="en_US.twitter.txt"
lineNews <- readLines(fileName, n=100000)

### NOTE: I SHOULD REMOVE PROFANITY AS WELL SOMEWHERE ###

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(lineNews, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

require(RTextTools)
myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeSparseTerms=0.60, 
                    removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#creates a matrix where terms are columns, documents are rows.
colnames(myTDM) #lists all terms
# ok above results in only 32 words!
myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)

# Could problem be that I did not load tm or tau?
# okay, that worked and produced 48399 columns

## VIEW ##
options(max.print=100000)
sink("myTDM.txt")
findFreqTerms(x = myTDM, lowfreq=1, highfreq=Inf)
sink()

# so it seems i have only unigrams, and that the reason may be that words have been mashed together for some reason. I will try again without all the other options.

myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=FALSE, stemWords=FALSE, ngramLength=c(2),  
                    removePunctuation=FALSE, removeStopwords=FALSE, stripWhitespace=FALSE, toLower=TRUE)
# Error in FUN(X[[2L]], ...) : non-character argument

debug(create_matrix)
undebug(create_matrix)
#OR debugonce(create_matrix)

capture.output(create_matrix(cleanData3, language="english", 
                    removeNumbers=FALSE, stemWords=FALSE, ngramLength=c(2),  
                    removePunctuation=FALSE, removeStopwords=FALSE, stripWhitespace=FALSE, toLower=TRUE), file='source_create_matrix.r')
```

```r
tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))
```
This creates the same error, therefore, the problem is within this, since create_matrix calls this.

```r
tokenize_ngrams("This is the first document.")
# [1] "is the first"       "the first document" "this is the"
```
so that works... 
 so the error is in the control= or tokenize= thing.
SO, the easiest way for me to work this i guess would be to simply tokenize ahead of time? 
I could create separate matrix for bigrams. But how to keep documents together.

# LET"S READ THE DOCS FOR tm CONTROL=
```r
matrix <- TermDocumentMatrix(corpus)
```
So the problem seems to be that a custom function for tokenize... well i guess it is allowed but perhaps this one doesn't do it right.

```r
library(rJava)
library(RWeka)
library(tm)

bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = bigramTokenizer))
```
Woot! that worked...

```r
library(RTextTools)
TDM2 <- create_matrix(corpus, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#Warning message:
#In if (ngramLength > 1) { :
  #the condition has length > 1 and only the first element will be used
```
So, that way does not work.

```r
library(rJava)
library(RWeka)
library(tm)

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
```
#*Sweet! That seems to do it.*#

#--#
'added today' %in% Terms(myTDM)
FALSE
findAssocs(myTDM,'know',0)

options(max.print=100000)
sink("myTDM.txt")		
inspect(myTDM)
sink()

```


# "case of" is not in the dataset en_US.news.txt

```r
'case of' %in% Terms(txtTdmBi)
FALSE
'added today' %in% Terms(txtTdmBi)
TRUE
findAssocs(txtTdmBi2, "added today", 0)

rowSums(as.matrix(txtTdmBi))
```

#OK let's see the top bigrams;

```r
findFreqTerms(x = txtTdmBi2, lowfreq=158, highfreq=Inf)
findAssocs(txtTdmBi2, "barack obama", 0.1)
```

# SPEED ISSUES

```r
library(gamlr)
findAssocsBig <- function(u, term, corlimit){
  suppressWarnings(x.cor <-  gamlr:corr(t(u[ !u$dimnames$Terms == term, ]),        
                                         as.matrix(t(u[  u$dimnames$Terms == term, ]))  ))  
  x <- sort(round(x.cor[(x.cor[, term] > corlimit), ], 2), decreasing = TRUE)
  return(x)
}
findAssocsBig(txtTdmBi, "added today", 0.01)
#Error in get(name, envir = asNamespace(pkg), inherits = FALSE) : 
#  object 'corr' not found
```
# large corpus finding nothing...

```r
findFreqTerms(x = tdm, lowfreq=1758, highfreq=Inf)
 [1] "also"    "back"    "call"    "can"     "citi"    "come"   
 [7] "compani" "counti"  "day"     "even"    "first"   "game"   
[13] "get"     "good"    "help"    "high"    "home"    "includ" 
[19] "just"    "know"    "last"    "like"    "look"    "made"   
[25] "make"    "mani"    "may"     "million" "month"   "much"   
[31] "need"    "new"     "now"     "one"     "open"    "peopl"  
[37] "percent" "plan"    "play"    "point"   "polic"   "public" 
[43] "report"  "right"   "run"     "said"    "say"     "school" 
[49] "season"  "see"     "show"    "sinc"    "start"   "state"  
[55] "still"   "take"    "team"    "think"   "three"   "time"   
[61] "tri"     "two"     "use"     "want"    "way"     "week"   
[67] "well"    "will"    "work"    "year"

findAssocs(tdm,"first",corlimit=0.5)
# how can this be 0... too many words?
findAssocs(tdm,"first",corlimit=0.95)
# nope
findAssocs(tdm,"first",corlimit=0.01)
# a ha, ok it's just that i have 77,000 documents and nothing shows up more than maybe 1758 times...
findAssocs(tdm,"first",corlimit=0.02)

#let's try this with "case"
findAssocs(tdm,"case",corlimit=0.03)
# everything is legal related. looks like bigrams will be needed.

```

# bigram
RWeka package
bigram tokenizer...

```r
#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
```

# Idea
Load the test sentence as a TDM.
Use findAssoc to get words?

```r
#Set working directory
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="testData.txt"
testData <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(testData, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
require(tm)
corpus <- Corpus(VectorSource(cleanData3))

## TOKENIZATION ## 

# REMOVE WHITESPACE:
corpus1 <- tm_map(corpus, stripWhitespace)
inspect(corpus1) #don't see a big difference

# LOWERCASE:
corpus2 <- tm_map(corpus1, content_transformer(tolower))
inspect(corpus2) #works

# REMOVE STOPWORDS
corpus3 <- tm_map(corpus2, removeWords, stopwords("english"))
inspect(corpus3) # ok the has been removed...

# STEMMING
corpus4 <- tm_map(corpus3, stemDocument)
inspect(corpus4) # Looks stemmed.

# REMOVE PUNCTUATION
corpus5<- tm_map(corpus4,removePunctuation)

# REMOVE NUMBERS
corpus6<- tm_map(corpus5,removeNumbers)

## END TOKENIZATION ##

## TOKEN ANALYSIS ##

# MAKE TERM DOCUMENT MATRIX (TDM) - a matrix of frequency counts for each word used in the corpus.
tdm<- TermDocumentMatrix(corpus6)
dtm<- DocumentTermMatrix(corpus6)
dtm

```

# Implement "Bag of Words" model

I need to weight the words.


[](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)

# SOME COMMANDS:

```r
findFreqTerms(x = tdm, lowfreq=58, highfreq=Inf)
findAssocs(x=tdm,term="also",corlimit=0.1)

# 1. take a sentence and iterate through the words

# 2. Look up associated words 

# 3. Give a score to each association with higher score to the words near the blank. 

## POSSIBILITIES:

# 1. REMOVE SPARSE TERMS
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
# [1] 18651   182
dim(TDM.common)
# [1]  71 182
```

# Quiz 2
## Question 1
The guy in front of me just bought a pound of bacon, a bouquet, and a case of
soda
pretzels
cheese
beer

# Vocab
*orthogonal* - right-angle but in statistics means not related.
*dot product* - an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.
*Euclidean space* - encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces
